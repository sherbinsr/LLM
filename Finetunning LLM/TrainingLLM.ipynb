{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9624f9-8005-430f-a8fe-a24ad454b7be",
   "metadata": {},
   "source": [
    "# Training LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfc4f6f-3b7c-424c-8716-557760e09fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lamini\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment variables\n",
    "api_key = os.getenv('API_KEY')\n",
    "os.environ[\"POWERML__PRODUCTION__KEY\"]= api_key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0614261b-6a01-47d6-ae99-bb9bf4fce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamini.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "81ea8dcf-25d2-44f5-b298-20a2ebf4de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28956a36-1505-4a12-8905-aa13b7ce822a",
   "metadata": {},
   "source": [
    "## Load the Lamini docs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9587be5-db4b-45d7-ba20-63b869888dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0f8bfa8-e7af-4a6c-a92d-90695e3b121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"lamini/lamini_docs\"\n",
    "use_hf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ae225-5bd5-4323-b904-dffd42a7a473",
   "metadata": {},
   "source": [
    "## Set up the model, training config, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25052a12-8f04-4f30-9dcd-03fdaab4719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37dfe618-459a-4e3a-8beb-0d2e2977032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26bb406-56ed-4d8b-ada9-8fd1e098ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8c675-2ca2-4266-9913-b6bdb155000d",
   "metadata": {},
   "source": [
    "## Load the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66f1f143-fc0a-4eb1-9672-8fe071efefae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370316cac71c461993e6bebbc5f5d8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherb\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sherb\\.cache\\huggingface\\hub\\models--EleutherAI--pythia-70m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c721355165b46d992b06e90900aba73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f55763f4-8273-4275-aa61-0af467f4cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8d10c1a-8f39-4cf9-9261-0555efa4c706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ffd25-645a-4a0c-9dfc-306999b1bf69",
   "metadata": {},
   "source": [
    "## Define function to carry out inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4ba363f8-53a4-41e0-a477-2dc1a1d16ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12992f1-5bdb-44b9-8857-8dc210a24af1",
   "metadata": {},
   "source": [
    "## Try the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c328cd9-32ec-4f78-b7cf-3c8a5874d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98787c54-2111-4cc4-987f-b5409d3f8618",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd283c7d-7611-41d0-8bc6-c45fd1a65772",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "41ffb4fc-a60f-4e7e-bb5d-e030022a6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fddc13ed-5ddb-4bec-b62e-518174139c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\sherb\\anaconda3\\lib\\site-packages (4.42.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.24.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.66.4)\n",
      "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
      "  Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2024.7.4)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sherb\\anaconda3\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "   ---------------------------------------- 0.0/314.1 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/314.1 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/314.1 kB 393.8 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 122.9/314.1 kB 901.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 256.0/314.1 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 314.1/314.1 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.32.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d932228-16b0-4dbf-a40d-1eca28127b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683eeb36-66c8-4288-a6db-47b14706cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedb82b-004d-488c-979d-7b5c2571dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71b99e-03de-4d6a-809b-b6bccf1a5b84",
   "metadata": {},
   "source": [
    "## Trian data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0a8a1-680a-494a-9326-d7c3ca5a95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3be6f5-af6e-46b9-8c9e-ed0350935550",
   "metadata": {},
   "source": [
    "## save the output locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33706b3-d030-4a34-a288-7082481ed3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e7328-b107-4807-8ba9-f13a288edd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157d1e0-7851-42e9-9e62-2623ab689a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ac82a-96e3-484c-94cc-5da728bdbd18",
   "metadata": {},
   "source": [
    "## Run slightly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bad1a0-e576-41fc-ba64-7da755afb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac8e13-9b76-446d-a6d1-e119b9940113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
